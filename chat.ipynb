{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4499  | total loss: \u001b[1m\u001b[32m0.94596\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1500 | loss: 0.94596 - acc: 0.9415 -- iter: 16/20\n",
      "Training Step: 4500  | total loss: \u001b[1m\u001b[32m0.85174\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1500 | loss: 0.85174 - acc: 0.9474 -- iter: 20/20\n",
      "--\n",
      "INFO:tensorflow:c:\\Users\\pt.yo-tatsuta\\Documents\\python\\python3.9\\ai\\deeplearning\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow\n",
    "\n",
    "import random\n",
    "import json\n",
    "\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "# jsonデータの読み込み\n",
    "with open(\"intents.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "# 読み込んだデータの抽出\n",
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        wrds = nltk.word_tokenize(pattern)\n",
    "        words.extend(wrds)\n",
    "        docs_x.append(wrds)\n",
    "        docs_y.append(intent[\"tag\"])\n",
    "        \n",
    "    if intent[\"tag\"] not in labels:\n",
    "        labels.append(intent[\"tag\"])\n",
    "\n",
    "\n",
    "# 単語のステミング\n",
    "words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "labels = sorted(labels)\n",
    "\n",
    "\n",
    "# bag of words\n",
    "training = []\n",
    "output = []\n",
    "\n",
    "out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "for x, doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "    \n",
    "    wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "    \n",
    "    for w in words:\n",
    "        if w in wrds:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "            \n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "    \n",
    "    training.append(bag)\n",
    "    output.append(output_row)\n",
    "        \n",
    "training = np.array(training)\n",
    "output = np.array(output)\n",
    "\n",
    "\n",
    "# モデル開発\n",
    "tensorflow.compat.v1.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "\n",
    "# モデルのトレーニングと保存\n",
    "model.fit(training, output, n_epoch=1500, batch_size=8, show_metric=True)\n",
    "model.save(\"model.tflearn\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start talking with your bot (type quit to stop)!\n",
      "We accept VISA, Mastercard and AMEX\n",
      "We accept VISA, Mastercard and AMEX\n",
      "Bye! Come back again soon.\n",
      "See you later, thanks for visiting\n",
      "Bye! Come back again soon.\n",
      "Bye! Come back again soon.\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "    \n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(words.lower()) for words in s_words]\n",
    "    \n",
    "    for se in s_words:\n",
    "        for i, q in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "    \n",
    "    return np.array(bag)\n",
    "\n",
    "def chat():\n",
    "    print(\"Start talking with your bot (type quit to stop)!\")\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "        \n",
    "        results = model.predict([bag_of_words(inp, words)])\n",
    "        results_index = np.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "        \n",
    "        for tg in data[\"intents\"]:\n",
    "            if tg[\"tag\"] == tag:\n",
    "                responses = tg[\"responses\"]\n",
    "                \n",
    "        print(random.choice(responses))\n",
    "        \n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e458654d6626132e9eb7b63c8d94e788e8b090fe2e768153e9fe03c71984ad1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
